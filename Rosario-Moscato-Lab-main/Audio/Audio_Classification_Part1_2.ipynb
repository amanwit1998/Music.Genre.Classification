{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"cardiovascular-study"},"source":["# Audio Classification \n","\n","Dataset here [https://urbansounddataset.weebly.com/](https://urbansounddataset.weebly.com/)"],"id":"cardiovascular-study"},{"cell_type":"markdown","metadata":{"id":"NPqtI5BBrZvG"},"source":["## Data Preprocessing"],"id":"NPqtI5BBrZvG"},{"cell_type":"markdown","metadata":{"id":"applicable-priest"},"source":["### Features Extraction\n"],"id":"applicable-priest"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mDbMppd3qg4c","outputId":"22459675-7923-4866-dc3f-02cf8fe5f4f0","executionInfo":{"status":"ok","timestamp":1679809033100,"user_tz":-330,"elapsed":19530,"user":{"displayName":"Amanwit Kumar","userId":"16746267613797614089"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"id":"mDbMppd3qg4c","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"liberal-journalist","executionInfo":{"status":"ok","timestamp":1679809039022,"user_tz":-330,"elapsed":402,"user":{"displayName":"Amanwit Kumar","userId":"16746267613797614089"}}},"source":["# Pkgs loading\n","import pandas as pd\n","import os\n","import librosa\n","import numpy as np\n","from tqdm import tqdm"],"id":"liberal-journalist","execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zf_xqeZJr3hR","colab":{"base_uri":"https://localhost:8080/","height":206},"outputId":"596d20ea-5ac2-434b-ac73-6dde63ec9cff","executionInfo":{"status":"ok","timestamp":1679810476348,"user_tz":-330,"elapsed":447,"user":{"displayName":"Amanwit Kumar","userId":"16746267613797614089"}}},"source":["audio_dataset_path='/content/drive/MyDrive/Capstone/UrbanSound8K/audio'\n","metadata=pd.read_csv('/content/drive/MyDrive/Capstone/UrbanSound8K/metadata/UrbanSound8K.csv')\n","metadata.head()"],"id":"Zf_xqeZJr3hR","execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["      slice_file_name    fsID  start        end  salience  fold  classID  \\\n","0    100032-3-0-0.wav  100032    0.0   0.317551         1     5        3   \n","1  100263-2-0-117.wav  100263   58.5  62.500000         1     5        2   \n","2  100263-2-0-121.wav  100263   60.5  64.500000         1     5        2   \n","3  100263-2-0-126.wav  100263   63.0  67.000000         1     5        2   \n","4  100263-2-0-137.wav  100263   68.5  72.500000         1     5        2   \n","\n","              class  \n","0          dog_bark  \n","1  children_playing  \n","2  children_playing  \n","3  children_playing  \n","4  children_playing  "],"text/html":["\n","  <div id=\"df-88273bfb-32e5-47b7-9c9d-c60dac7b1e9b\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>slice_file_name</th>\n","      <th>fsID</th>\n","      <th>start</th>\n","      <th>end</th>\n","      <th>salience</th>\n","      <th>fold</th>\n","      <th>classID</th>\n","      <th>class</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>100032-3-0-0.wav</td>\n","      <td>100032</td>\n","      <td>0.0</td>\n","      <td>0.317551</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>3</td>\n","      <td>dog_bark</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>100263-2-0-117.wav</td>\n","      <td>100263</td>\n","      <td>58.5</td>\n","      <td>62.500000</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>2</td>\n","      <td>children_playing</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>100263-2-0-121.wav</td>\n","      <td>100263</td>\n","      <td>60.5</td>\n","      <td>64.500000</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>2</td>\n","      <td>children_playing</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>100263-2-0-126.wav</td>\n","      <td>100263</td>\n","      <td>63.0</td>\n","      <td>67.000000</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>2</td>\n","      <td>children_playing</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>100263-2-0-137.wav</td>\n","      <td>100263</td>\n","      <td>68.5</td>\n","      <td>72.500000</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>2</td>\n","      <td>children_playing</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-88273bfb-32e5-47b7-9c9d-c60dac7b1e9b')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-88273bfb-32e5-47b7-9c9d-c60dac7b1e9b button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-88273bfb-32e5-47b7-9c9d-c60dac7b1e9b');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"S7ygvg0_xoDL","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c821b344-cd72-46cf-ff5f-d174ce31ea95","executionInfo":{"status":"ok","timestamp":1679810479215,"user_tz":-330,"elapsed":413,"user":{"displayName":"Amanwit Kumar","userId":"16746267613797614089"}}},"source":["# Dataset Balancing/Imbalancing Check\n","metadata['class'].value_counts()"],"id":"S7ygvg0_xoDL","execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dog_bark            1000\n","children_playing    1000\n","air_conditioner     1000\n","street_music        1000\n","engine_idling       1000\n","jackhammer          1000\n","drilling            1000\n","siren                929\n","car_horn             429\n","gun_shot             374\n","Name: class, dtype: int64"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","metadata":{"id":"chinese-calendar","executionInfo":{"status":"ok","timestamp":1679811806107,"user_tz":-330,"elapsed":514,"user":{"displayName":"Amanwit Kumar","userId":"16746267613797614089"}}},"source":["# Extracting MFCC's For every audio file\n","def features_extractor(file_name):\n","    audio, sample_rate = librosa.load(file_name, sr=None, res_type='kaiser_fast')\n","    mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n","    mfccs_scaled_features = np.mean(mfccs_features.T,axis=0)\n","    \n","    return mfccs_scaled_features"],"id":"chinese-calendar","execution_count":35,"outputs":[]},{"cell_type":"code","metadata":{"id":"nuclear-sponsorship","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8a9923d9-bb10-42b3-e458-2d9c09b2c129","executionInfo":{"status":"ok","timestamp":1679814083059,"user_tz":-330,"elapsed":2275172,"user":{"displayName":"Amanwit Kumar","userId":"16746267613797614089"}}},"source":["# Features extraction from all audio files (MFCC)\n","extracted_features=[]\n","for index_num,row in tqdm(metadata.iterrows()):\n","    file_name = os.path.join(os.path.abspath(audio_dataset_path),'fold'+str(row[\"fold\"])+'/',str(row[\"slice_file_name\"]))\n","    final_class_labels=row[\"class\"]\n","    data=features_extractor(file_name)\n","    extracted_features.append([data,final_class_labels])"],"id":"nuclear-sponsorship","execution_count":36,"outputs":[{"output_type":"stream","name":"stderr","text":["2850it [12:35,  3.25it/s]/usr/local/lib/python3.9/dist-packages/librosa/feature/spectral.py:2157: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n","  mel_basis = filters.mel(sr=sr, n_fft=n_fft, **kwargs)\n","8732it [37:54,  3.84it/s]\n"]}]},{"cell_type":"code","metadata":{"id":"acoustic-wagner","colab":{"base_uri":"https://localhost:8080/","height":206},"outputId":"b1784d06-7500-4200-cf2b-4a2afbdb4bc4","executionInfo":{"status":"ok","timestamp":1679814145972,"user_tz":-330,"elapsed":437,"user":{"displayName":"Amanwit Kumar","userId":"16746267613797614089"}}},"source":["# Converting extracted_features to Pandas dataframe\n","extracted_features_df=pd.DataFrame(extracted_features,columns=['feature','class'])\n","extracted_features_df.head()"],"id":"acoustic-wagner","execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                             feature             class\n","0  [-275.91843, 119.4928, -98.21178, -66.51513, -...          dog_bark\n","1  [-500.9084, 185.10641, -86.53282, 49.85885, 9....  children_playing\n","2  [-531.1953, 186.93994, -70.34916, 40.429245, 9...  children_playing\n","3  [-476.78442, 160.33328, -62.952843, 50.75117, ...  children_playing\n","4  [-521.2447, 185.39265, -81.95048, 46.47355, 11...  children_playing"],"text/html":["\n","  <div id=\"df-b6a05f44-b075-44ac-99ce-4fb5eb94b4fa\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>feature</th>\n","      <th>class</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[-275.91843, 119.4928, -98.21178, -66.51513, -...</td>\n","      <td>dog_bark</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[-500.9084, 185.10641, -86.53282, 49.85885, 9....</td>\n","      <td>children_playing</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[-531.1953, 186.93994, -70.34916, 40.429245, 9...</td>\n","      <td>children_playing</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>[-476.78442, 160.33328, -62.952843, 50.75117, ...</td>\n","      <td>children_playing</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>[-521.2447, 185.39265, -81.95048, 46.47355, 11...</td>\n","      <td>children_playing</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b6a05f44-b075-44ac-99ce-4fb5eb94b4fa')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-b6a05f44-b075-44ac-99ce-4fb5eb94b4fa button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-b6a05f44-b075-44ac-99ce-4fb5eb94b4fa');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":37}]},{"cell_type":"code","metadata":{"id":"jC2WMna1xbsi","executionInfo":{"status":"ok","timestamp":1679814177946,"user_tz":-330,"elapsed":3535,"user":{"displayName":"Amanwit Kumar","userId":"16746267613797614089"}}},"source":["# Data Frame Saving\n","extracted_features_df.to_csv(\"UrbanSound8K_DF.csv\")"],"id":"jC2WMna1xbsi","execution_count":38,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8Ak54TtVsgKL"},"source":["### Data Splitting and encoding"],"id":"8Ak54TtVsgKL"},{"cell_type":"code","metadata":{"id":"characteristic-sudan","executionInfo":{"status":"ok","timestamp":1679814268308,"user_tz":-330,"elapsed":541,"user":{"displayName":"Amanwit Kumar","userId":"16746267613797614089"}}},"source":["# Data Splitting\n","X=np.array(extracted_features_df['feature'].tolist())\n","y=np.array(extracted_features_df['class'].tolist())"],"id":"characteristic-sudan","execution_count":39,"outputs":[]},{"cell_type":"code","metadata":{"id":"friendly-placement","colab":{"base_uri":"https://localhost:8080/"},"outputId":"250d7002-d906-4da2-d9cd-d5727d6f7c2a","executionInfo":{"status":"ok","timestamp":1679814273662,"user_tz":-330,"elapsed":2,"user":{"displayName":"Amanwit Kumar","userId":"16746267613797614089"}}},"source":["X.shape"],"id":"friendly-placement","execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(8732, 40)"]},"metadata":{},"execution_count":40}]},{"cell_type":"code","metadata":{"id":"japanese-cheese","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f3bd49c2-912b-456b-c2d5-a0588a260fb7","executionInfo":{"status":"ok","timestamp":1679814277285,"user_tz":-330,"elapsed":487,"user":{"displayName":"Amanwit Kumar","userId":"16746267613797614089"}}},"source":["y"],"id":"japanese-cheese","execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['dog_bark', 'children_playing', 'children_playing', ...,\n","       'car_horn', 'car_horn', 'car_horn'], dtype='<U16')"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","metadata":{"id":"wired-church","executionInfo":{"status":"ok","timestamp":1679814289619,"user_tz":-330,"elapsed":10133,"user":{"displayName":"Amanwit Kumar","userId":"16746267613797614089"}}},"source":["# Label Encoding\n","from tensorflow.keras.utils import to_categorical\n","from sklearn.preprocessing import LabelEncoder\n","labelencoder=LabelEncoder()\n","y=to_categorical(labelencoder.fit_transform(y))"],"id":"wired-church","execution_count":42,"outputs":[]},{"cell_type":"code","metadata":{"id":"sitting-anniversary","colab":{"base_uri":"https://localhost:8080/"},"outputId":"bce4438a-55a0-4737-c69b-4fb0421cca61","executionInfo":{"status":"ok","timestamp":1679814292241,"user_tz":-330,"elapsed":4,"user":{"displayName":"Amanwit Kumar","userId":"16746267613797614089"}}},"source":["y"],"id":"sitting-anniversary","execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 1., ..., 0., 0., 0.],\n","       [0., 0., 1., ..., 0., 0., 0.],\n","       ...,\n","       [0., 1., 0., ..., 0., 0., 0.],\n","       [0., 1., 0., ..., 0., 0., 0.],\n","       [0., 1., 0., ..., 0., 0., 0.]], dtype=float32)"]},"metadata":{},"execution_count":43}]},{"cell_type":"code","metadata":{"id":"documentary-priority","executionInfo":{"status":"ok","timestamp":1679814294784,"user_tz":-330,"elapsed":403,"user":{"displayName":"Amanwit Kumar","userId":"16746267613797614089"}}},"source":["# Training Testing Sets\n","from sklearn.model_selection import train_test_split\n","X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)"],"id":"documentary-priority","execution_count":44,"outputs":[]},{"cell_type":"code","metadata":{"id":"analyzed-payday","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4b2e9f60-256c-434b-8c53-cf33ff7b2c8f","executionInfo":{"status":"ok","timestamp":1679814296665,"user_tz":-330,"elapsed":3,"user":{"displayName":"Amanwit Kumar","userId":"16746267613797614089"}}},"source":["X_train"],"id":"analyzed-payday","execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[-1.8378271e+02,  1.5365129e+02, -3.6178783e+01, ...,\n","         1.0025054e+00, -4.7213894e-01, -1.2038720e+00],\n","       [-8.4152161e+01,  1.6649818e+02, -6.1209232e+01, ...,\n","        -4.0521545e+00, -3.3977334e+00, -8.2560587e+00],\n","       [-6.7970741e+01,  2.6627291e+01, -4.8024197e+01, ...,\n","        -4.1831975e+00,  2.3861418e+00,  4.4640236e+00],\n","       ...,\n","       [-4.7486859e+02,  9.3468300e+01,  3.2284161e+01, ...,\n","        -1.1071193e+00, -1.5656761e+00, -9.8130518e-01],\n","       [-2.0102074e+02,  1.7294638e+02, -3.5023716e+01, ...,\n","         4.1071025e-01, -1.1205076e+00,  4.3810743e-01],\n","       [-4.8265601e+02,  2.3059308e+02,  2.8331995e+01, ...,\n","         7.3604614e-01, -1.9397887e+00, -4.5186167e+00]], dtype=float32)"]},"metadata":{},"execution_count":45}]},{"cell_type":"code","metadata":{"id":"flexible-lithuania","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8b364d53-47ae-491f-92fe-72923d92d3cc","executionInfo":{"status":"ok","timestamp":1679814299340,"user_tz":-330,"elapsed":430,"user":{"displayName":"Amanwit Kumar","userId":"16746267613797614089"}}},"source":["X_train.shape"],"id":"flexible-lithuania","execution_count":46,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(6985, 40)"]},"metadata":{},"execution_count":46}]},{"cell_type":"code","metadata":{"id":"instrumental-equity","colab":{"base_uri":"https://localhost:8080/"},"outputId":"616a617b-5c1d-449f-8d00-8c8680d05a3b","executionInfo":{"status":"ok","timestamp":1679814301260,"user_tz":-330,"elapsed":2,"user":{"displayName":"Amanwit Kumar","userId":"16746267613797614089"}}},"source":["X_test.shape"],"id":"instrumental-equity","execution_count":47,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1747, 40)"]},"metadata":{},"execution_count":47}]},{"cell_type":"code","metadata":{"id":"chemical-vermont","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9aee4a3e-a56a-4315-8811-3c9509a0f79e","executionInfo":{"status":"ok","timestamp":1679814303390,"user_tz":-330,"elapsed":3,"user":{"displayName":"Amanwit Kumar","userId":"16746267613797614089"}}},"source":["y_train.shape"],"id":"chemical-vermont","execution_count":48,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(6985, 10)"]},"metadata":{},"execution_count":48}]},{"cell_type":"code","metadata":{"id":"governing-natural","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a7387e28-b45f-4839-e776-3fe435fcc9c3","executionInfo":{"status":"ok","timestamp":1679814306413,"user_tz":-330,"elapsed":3,"user":{"displayName":"Amanwit Kumar","userId":"16746267613797614089"}}},"source":["y_test.shape"],"id":"governing-natural","execution_count":49,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1747, 10)"]},"metadata":{},"execution_count":49}]},{"cell_type":"markdown","metadata":{"id":"BgKF10EaWB_U"},"source":["### Model Creation"],"id":"BgKF10EaWB_U"},{"cell_type":"code","metadata":{"id":"dqkR5bqPBMBQ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1c0759d3-69d2-456a-a2a7-b6315812ffec","executionInfo":{"status":"ok","timestamp":1679814309688,"user_tz":-330,"elapsed":2,"user":{"displayName":"Amanwit Kumar","userId":"16746267613797614089"}}},"source":["import tensorflow as tf\n","print(tf.__version__)"],"id":"dqkR5bqPBMBQ","execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["2.11.0\n"]}]},{"cell_type":"code","metadata":{"id":"1UnfDKJsWK19","executionInfo":{"status":"ok","timestamp":1679814312860,"user_tz":-330,"elapsed":2,"user":{"displayName":"Amanwit Kumar","userId":"16746267613797614089"}}},"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense,Dropout,Activation\n","from sklearn import metrics"],"id":"1UnfDKJsWK19","execution_count":51,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BufirgXwWOIk","outputId":"6117d12c-3ef7-4486-bf74-913b9e2a868d","executionInfo":{"status":"ok","timestamp":1679814316345,"user_tz":-330,"elapsed":1,"user":{"displayName":"Amanwit Kumar","userId":"16746267613797614089"}}},"source":["# No of classes\n","num_labels=y.shape[1]\n","print(num_labels)"],"id":"BufirgXwWOIk","execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["10\n"]}]},{"cell_type":"code","metadata":{"id":"P18Z7qE0WzI1","executionInfo":{"status":"ok","timestamp":1679814344465,"user_tz":-330,"elapsed":1634,"user":{"displayName":"Amanwit Kumar","userId":"16746267613797614089"}}},"source":["model=Sequential()\n","\n","#first layer\n","model.add(Dense(1600,input_shape=(40,)))\n","model.add(Activation('relu'))\n","model.add(Dropout(0.5))\n","\n","#second layer\n","model.add(Dense(800))\n","model.add(Activation('relu'))\n","model.add(Dropout(0.5))\n","\n","#third layer\n","model.add(Dense(400))\n","model.add(Activation('relu'))\n","model.add(Dropout(0.5))\n","\n","#final layer\n","model.add(Dense(num_labels))\n","model.add(Activation('softmax'))"],"id":"P18Z7qE0WzI1","execution_count":53,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2uzAtNGQXzNu","outputId":"9e6d8a99-5a67-436e-91c4-22c292a36321","executionInfo":{"status":"ok","timestamp":1679814349359,"user_tz":-330,"elapsed":528,"user":{"displayName":"Amanwit Kumar","userId":"16746267613797614089"}}},"source":["model.summary()"],"id":"2uzAtNGQXzNu","execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense (Dense)               (None, 1600)              65600     \n","                                                                 \n"," activation (Activation)     (None, 1600)              0         \n","                                                                 \n"," dropout (Dropout)           (None, 1600)              0         \n","                                                                 \n"," dense_1 (Dense)             (None, 800)               1280800   \n","                                                                 \n"," activation_1 (Activation)   (None, 800)               0         \n","                                                                 \n"," dropout_1 (Dropout)         (None, 800)               0         \n","                                                                 \n"," dense_2 (Dense)             (None, 400)               320400    \n","                                                                 \n"," activation_2 (Activation)   (None, 400)               0         \n","                                                                 \n"," dropout_2 (Dropout)         (None, 400)               0         \n","                                                                 \n"," dense_3 (Dense)             (None, 10)                4010      \n","                                                                 \n"," activation_3 (Activation)   (None, 10)                0         \n","                                                                 \n","=================================================================\n","Total params: 1,670,810\n","Trainable params: 1,670,810\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","metadata":{"id":"nTIirDNfX4V_","executionInfo":{"status":"ok","timestamp":1679814354691,"user_tz":-330,"elapsed":451,"user":{"displayName":"Amanwit Kumar","userId":"16746267613797614089"}}},"source":["model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')"],"id":"nTIirDNfX4V_","execution_count":55,"outputs":[]},{"cell_type":"code","metadata":{"id":"4A8jg3f0X86z","executionInfo":{"status":"ok","timestamp":1679814357208,"user_tz":-330,"elapsed":2,"user":{"displayName":"Amanwit Kumar","userId":"16746267613797614089"}}},"source":["# Model training\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from datetime import datetime "],"id":"4A8jg3f0X86z","execution_count":56,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u0e36g3_YDVD","outputId":"97478bf3-fe85-4e77-bdc4-9db5cf68c879","executionInfo":{"status":"ok","timestamp":1679814807168,"user_tz":-330,"elapsed":443851,"user":{"displayName":"Amanwit Kumar","userId":"16746267613797614089"}}},"source":["num_epochs = 100\n","num_batch_size = 128\n","\n","checkpointer = ModelCheckpoint(filepath='saved_models/audio_classification.h5', verbose=1, save_best_only=True)\n","start = datetime.now()\n","\n","model.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(X_test, y_test), callbacks=[checkpointer], verbose=1)\n","\n","\n","duration = datetime.now() - start\n","print(\"Training completed in time: \", duration)"],"id":"u0e36g3_YDVD","execution_count":57,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","54/55 [============================>.] - ETA: 0s - loss: 7.8781 - accuracy: 0.1589\n","Epoch 1: val_loss improved from inf to 2.10538, saving model to saved_models/audio_classification.h5\n","55/55 [==============================] - 9s 138ms/step - loss: 7.8234 - accuracy: 0.1595 - val_loss: 2.1054 - val_accuracy: 0.2335\n","Epoch 2/100\n","54/55 [============================>.] - ETA: 0s - loss: 2.3676 - accuracy: 0.2188\n","Epoch 2: val_loss improved from 2.10538 to 1.94197, saving model to saved_models/audio_classification.h5\n","55/55 [==============================] - 3s 63ms/step - loss: 2.3665 - accuracy: 0.2183 - val_loss: 1.9420 - val_accuracy: 0.3875\n","Epoch 3/100\n","54/55 [============================>.] - ETA: 0s - loss: 2.1098 - accuracy: 0.2714\n","Epoch 3: val_loss improved from 1.94197 to 1.90159, saving model to saved_models/audio_classification.h5\n","55/55 [==============================] - 3s 62ms/step - loss: 2.1077 - accuracy: 0.2720 - val_loss: 1.9016 - val_accuracy: 0.3818\n","Epoch 4/100\n","55/55 [==============================] - ETA: 0s - loss: 1.9455 - accuracy: 0.3175\n","Epoch 4: val_loss improved from 1.90159 to 1.69576, saving model to saved_models/audio_classification.h5\n","55/55 [==============================] - 7s 127ms/step - loss: 1.9455 - accuracy: 0.3175 - val_loss: 1.6958 - val_accuracy: 0.4757\n","Epoch 5/100\n","55/55 [==============================] - ETA: 0s - loss: 1.8047 - accuracy: 0.3711\n","Epoch 5: val_loss improved from 1.69576 to 1.57962, saving model to saved_models/audio_classification.h5\n","55/55 [==============================] - 6s 101ms/step - loss: 1.8047 - accuracy: 0.3711 - val_loss: 1.5796 - val_accuracy: 0.5112\n","Epoch 6/100\n","55/55 [==============================] - ETA: 0s - loss: 1.6649 - accuracy: 0.4140\n","Epoch 6: val_loss improved from 1.57962 to 1.41819, saving model to saved_models/audio_classification.h5\n","55/55 [==============================] - 5s 82ms/step - loss: 1.6649 - accuracy: 0.4140 - val_loss: 1.4182 - val_accuracy: 0.5541\n","Epoch 7/100\n","55/55 [==============================] - ETA: 0s - loss: 1.5434 - accuracy: 0.4691\n","Epoch 7: val_loss improved from 1.41819 to 1.34665, saving model to saved_models/audio_classification.h5\n","55/55 [==============================] - 6s 115ms/step - loss: 1.5434 - accuracy: 0.4691 - val_loss: 1.3467 - val_accuracy: 0.5959\n","Epoch 8/100\n","54/55 [============================>.] - ETA: 0s - loss: 1.4598 - accuracy: 0.5016\n","Epoch 8: val_loss improved from 1.34665 to 1.28105, saving model to saved_models/audio_classification.h5\n","55/55 [==============================] - 5s 88ms/step - loss: 1.4582 - accuracy: 0.5018 - val_loss: 1.2810 - val_accuracy: 0.6228\n","Epoch 9/100\n","54/55 [============================>.] - ETA: 0s - loss: 1.3694 - accuracy: 0.5288\n","Epoch 9: val_loss improved from 1.28105 to 1.16916, saving model to saved_models/audio_classification.h5\n","55/55 [==============================] - 3s 62ms/step - loss: 1.3693 - accuracy: 0.5290 - val_loss: 1.1692 - val_accuracy: 0.6411\n","Epoch 10/100\n","54/55 [============================>.] - ETA: 0s - loss: 1.2753 - accuracy: 0.5666\n","Epoch 10: val_loss improved from 1.16916 to 1.07382, saving model to saved_models/audio_classification.h5\n","55/55 [==============================] - 3s 60ms/step - loss: 1.2758 - accuracy: 0.5661 - val_loss: 1.0738 - val_accuracy: 0.6789\n","Epoch 11/100\n","55/55 [==============================] - ETA: 0s - loss: 1.2009 - accuracy: 0.5956\n","Epoch 11: val_loss improved from 1.07382 to 1.01367, saving model to saved_models/audio_classification.h5\n","55/55 [==============================] - 4s 77ms/step - loss: 1.2009 - accuracy: 0.5956 - val_loss: 1.0137 - val_accuracy: 0.7023\n","Epoch 12/100\n","54/55 [============================>.] - ETA: 0s - loss: 1.1443 - accuracy: 0.6131\n","Epoch 12: val_loss improved from 1.01367 to 0.94558, saving model to saved_models/audio_classification.h5\n","55/55 [==============================] - 4s 79ms/step - loss: 1.1445 - accuracy: 0.6140 - val_loss: 0.9456 - val_accuracy: 0.7252\n","Epoch 13/100\n","54/55 [============================>.] - ETA: 0s - loss: 1.0793 - accuracy: 0.6387\n","Epoch 13: val_loss improved from 0.94558 to 0.86976, saving model to saved_models/audio_classification.h5\n","55/55 [==============================] - 3s 59ms/step - loss: 1.0793 - accuracy: 0.6385 - val_loss: 0.8698 - val_accuracy: 0.7378\n","Epoch 14/100\n","54/55 [============================>.] - ETA: 0s - loss: 1.0160 - accuracy: 0.6635\n","Epoch 14: val_loss improved from 0.86976 to 0.82613, saving model to saved_models/audio_classification.h5\n","55/55 [==============================] - 3s 61ms/step - loss: 1.0147 - accuracy: 0.6637 - val_loss: 0.8261 - val_accuracy: 0.7539\n","Epoch 15/100\n","55/55 [==============================] - ETA: 0s - loss: 0.9554 - accuracy: 0.6806\n","Epoch 15: val_loss improved from 0.82613 to 0.78045, saving model to saved_models/audio_classification.h5\n","55/55 [==============================] - 4s 74ms/step - loss: 0.9554 - accuracy: 0.6806 - val_loss: 0.7805 - val_accuracy: 0.7745\n","Epoch 16/100\n","54/55 [============================>.] - ETA: 0s - loss: 0.9149 - accuracy: 0.6999\n","Epoch 16: val_loss improved from 0.78045 to 0.74022, saving model to saved_models/audio_classification.h5\n","55/55 [==============================] - 4s 77ms/step - loss: 0.9137 - accuracy: 0.6999 - val_loss: 0.7402 - val_accuracy: 0.7647\n","Epoch 17/100\n","54/55 [============================>.] - ETA: 0s - loss: 0.8819 - accuracy: 0.7063\n","Epoch 17: val_loss improved from 0.74022 to 0.73894, saving model to saved_models/audio_classification.h5\n","55/55 [==============================] - 3s 59ms/step - loss: 0.8820 - accuracy: 0.7058 - val_loss: 0.7389 - val_accuracy: 0.7785\n","Epoch 18/100\n","54/55 [============================>.] - ETA: 0s - loss: 0.8536 - accuracy: 0.7209\n","Epoch 18: val_loss improved from 0.73894 to 0.69785, saving model to saved_models/audio_classification.h5\n","55/55 [==============================] - 3s 61ms/step - loss: 0.8533 - accuracy: 0.7211 - val_loss: 0.6978 - val_accuracy: 0.7853\n","Epoch 19/100\n","55/55 [==============================] - ETA: 0s - loss: 0.8094 - accuracy: 0.7297\n","Epoch 19: val_loss improved from 0.69785 to 0.64843, saving model to saved_models/audio_classification.h5\n","55/55 [==============================] - 4s 76ms/step - loss: 0.8094 - accuracy: 0.7297 - val_loss: 0.6484 - val_accuracy: 0.7859\n","Epoch 20/100\n","54/55 [============================>.] - ETA: 0s - loss: 0.7782 - accuracy: 0.7413\n","Epoch 20: val_loss improved from 0.64843 to 0.60134, saving model to saved_models/audio_classification.h5\n","55/55 [==============================] - 9s 160ms/step - loss: 0.7772 - accuracy: 0.7414 - val_loss: 0.6013 - val_accuracy: 0.8008\n","Epoch 21/100\n","54/55 [============================>.] - ETA: 0s - loss: 0.7509 - accuracy: 0.7503\n","Epoch 21: val_loss did not improve from 0.60134\n","55/55 [==============================] - 3s 58ms/step - loss: 0.7510 - accuracy: 0.7500 - val_loss: 0.6220 - val_accuracy: 0.8100\n","Epoch 22/100\n","54/55 [============================>.] - ETA: 0s - loss: 0.7148 - accuracy: 0.7626\n","Epoch 22: val_loss improved from 0.60134 to 0.58325, saving model to saved_models/audio_classification.h5\n","55/55 [==============================] - 4s 64ms/step - loss: 0.7169 - accuracy: 0.7621 - val_loss: 0.5833 - val_accuracy: 0.8185\n","Epoch 23/100\n","55/55 [==============================] - ETA: 0s - loss: 0.6778 - accuracy: 0.7739\n","Epoch 23: val_loss improved from 0.58325 to 0.53335, saving model to saved_models/audio_classification.h5\n","55/55 [==============================] - 5s 83ms/step - loss: 0.6778 - accuracy: 0.7739 - val_loss: 0.5334 - val_accuracy: 0.8443\n","Epoch 24/100\n","54/55 [============================>.] - ETA: 0s - loss: 0.6514 - accuracy: 0.7788\n","Epoch 24: val_loss improved from 0.53335 to 0.52714, saving model to saved_models/audio_classification.h5\n","55/55 [==============================] - 4s 77ms/step - loss: 0.6556 - accuracy: 0.7784 - val_loss: 0.5271 - val_accuracy: 0.8420\n","Epoch 25/100\n","54/55 [============================>.] - ETA: 0s - loss: 0.6368 - accuracy: 0.7885\n","Epoch 25: val_loss improved from 0.52714 to 0.51579, saving model to saved_models/audio_classification.h5\n","55/55 [==============================] - 3s 63ms/step - loss: 0.6384 - accuracy: 0.7878 - val_loss: 0.5158 - val_accuracy: 0.8414\n","Epoch 26/100\n","54/55 [============================>.] - ETA: 0s - loss: 0.6299 - accuracy: 0.7875\n","Epoch 26: val_loss improved from 0.51579 to 0.49797, saving model to saved_models/audio_classification.h5\n","55/55 [==============================] - 3s 60ms/step - loss: 0.6312 - accuracy: 0.7873 - val_loss: 0.4980 - val_accuracy: 0.8403\n","Epoch 27/100\n","55/55 [==============================] - ETA: 0s - loss: 0.5860 - accuracy: 0.7997\n","Epoch 27: val_loss improved from 0.49797 to 0.48319, saving model to saved_models/audio_classification.h5\n","55/55 [==============================] - 5s 84ms/step - loss: 0.5860 - accuracy: 0.7997 - val_loss: 0.4832 - val_accuracy: 0.8500\n","Epoch 28/100\n","55/55 [==============================] - ETA: 0s - loss: 0.5772 - accuracy: 0.8040\n","Epoch 28: val_loss improved from 0.48319 to 0.46389, saving model to saved_models/audio_classification.h5\n","55/55 [==============================] - 4s 72ms/step - loss: 0.5772 - accuracy: 0.8040 - val_loss: 0.4639 - val_accuracy: 0.8535\n","Epoch 29/100\n","54/55 [============================>.] - ETA: 0s - loss: 0.5744 - accuracy: 0.8073\n","Epoch 29: val_loss did not improve from 0.46389\n","55/55 [==============================] - 3s 63ms/step - loss: 0.5737 - accuracy: 0.8073 - val_loss: 0.4704 - val_accuracy: 0.8586\n","Epoch 30/100\n","54/55 [============================>.] - ETA: 0s - loss: 0.5471 - accuracy: 0.8105\n","Epoch 30: val_loss improved from 0.46389 to 0.45214, saving model to saved_models/audio_classification.h5\n","55/55 [==============================] - 4s 64ms/step - loss: 0.5487 - accuracy: 0.8099 - val_loss: 0.4521 - val_accuracy: 0.8649\n","Epoch 31/100\n","55/55 [==============================] - ETA: 0s - loss: 0.5351 - accuracy: 0.8189\n","Epoch 31: val_loss did not improve from 0.45214\n","55/55 [==============================] - 5s 85ms/step - loss: 0.5351 - accuracy: 0.8189 - val_loss: 0.4634 - val_accuracy: 0.8563\n","Epoch 32/100\n","54/55 [============================>.] - ETA: 0s - loss: 0.5157 - accuracy: 0.8247\n","Epoch 32: val_loss improved from 0.45214 to 0.43049, saving model to saved_models/audio_classification.h5\n","55/55 [==============================] - 4s 73ms/step - loss: 0.5174 - accuracy: 0.8241 - val_loss: 0.4305 - val_accuracy: 0.8729\n","Epoch 33/100\n","54/55 [============================>.] - ETA: 0s - loss: 0.5037 - accuracy: 0.8280\n","Epoch 33: val_loss did not improve from 0.43049\n","55/55 [==============================] - 3s 59ms/step - loss: 0.5034 - accuracy: 0.8281 - val_loss: 0.4497 - val_accuracy: 0.8506\n","Epoch 34/100\n","54/55 [============================>.] - ETA: 0s - loss: 0.5144 - accuracy: 0.8267\n","Epoch 34: val_loss improved from 0.43049 to 0.41086, saving model to saved_models/audio_classification.h5\n","55/55 [==============================] - 3s 60ms/step - loss: 0.5128 - accuracy: 0.8273 - val_loss: 0.4109 - val_accuracy: 0.8746\n","Epoch 35/100\n","55/55 [==============================] - ETA: 0s - loss: 0.4842 - accuracy: 0.8401\n","Epoch 35: val_loss improved from 0.41086 to 0.40007, saving model to saved_models/audio_classification.h5\n","55/55 [==============================] - 5s 90ms/step - loss: 0.4842 - accuracy: 0.8401 - val_loss: 0.4001 - val_accuracy: 0.8781\n","Epoch 36/100\n","54/55 [============================>.] - ETA: 0s - loss: 0.4791 - accuracy: 0.8375\n","Epoch 36: val_loss did not improve from 0.40007\n","55/55 [==============================] - 4s 70ms/step - loss: 0.4773 - accuracy: 0.8379 - val_loss: 0.4072 - val_accuracy: 0.8781\n","Epoch 37/100\n","54/55 [============================>.] - ETA: 0s - loss: 0.4641 - accuracy: 0.8396\n","Epoch 37: val_loss improved from 0.40007 to 0.38571, saving model to saved_models/audio_classification.h5\n","55/55 [==============================] - 3s 61ms/step - loss: 0.4630 - accuracy: 0.8397 - val_loss: 0.3857 - val_accuracy: 0.8849\n","Epoch 38/100\n","54/55 [============================>.] - ETA: 0s - loss: 0.4575 - accuracy: 0.8426\n","Epoch 38: val_loss improved from 0.38571 to 0.38489, saving model to saved_models/audio_classification.h5\n","55/55 [==============================] - 3s 59ms/step - loss: 0.4561 - accuracy: 0.8431 - val_loss: 0.3849 - val_accuracy: 0.8821\n","Epoch 39/100\n","55/55 [==============================] - ETA: 0s - loss: 0.4398 - accuracy: 0.8513\n","Epoch 39: val_loss did not improve from 0.38489\n","55/55 [==============================] - 5s 91ms/step - loss: 0.4398 - accuracy: 0.8513 - val_loss: 0.3952 - val_accuracy: 0.8804\n","Epoch 40/100\n","54/55 [============================>.] - ETA: 0s - loss: 0.4235 - accuracy: 0.8530\n","Epoch 40: val_loss improved from 0.38489 to 0.37162, saving model to saved_models/audio_classification.h5\n","55/55 [==============================] - 4s 71ms/step - loss: 0.4234 - accuracy: 0.8534 - val_loss: 0.3716 - val_accuracy: 0.8827\n","Epoch 41/100\n","54/55 [============================>.] - ETA: 0s - loss: 0.4305 - accuracy: 0.8524\n","Epoch 41: val_loss did not improve from 0.37162\n","55/55 [==============================] - 3s 60ms/step - loss: 0.4313 - accuracy: 0.8525 - val_loss: 0.3755 - val_accuracy: 0.8861\n","Epoch 42/100\n","54/55 [============================>.] - ETA: 0s - loss: 0.4178 - accuracy: 0.8563\n","Epoch 42: val_loss improved from 0.37162 to 0.36742, saving model to saved_models/audio_classification.h5\n","55/55 [==============================] - 3s 64ms/step - loss: 0.4174 - accuracy: 0.8565 - val_loss: 0.3674 - val_accuracy: 0.8849\n","Epoch 43/100\n","55/55 [==============================] - ETA: 0s - loss: 0.4224 - accuracy: 0.8547\n","Epoch 43: val_loss improved from 0.36742 to 0.35666, saving model to saved_models/audio_classification.h5\n","55/55 [==============================] - 5s 94ms/step - loss: 0.4224 - accuracy: 0.8547 - val_loss: 0.3567 - val_accuracy: 0.8849\n","Epoch 44/100\n","54/55 [============================>.] - ETA: 0s - loss: 0.4111 - accuracy: 0.8615\n","Epoch 44: val_loss did not improve from 0.35666\n","55/55 [==============================] - 3s 62ms/step - loss: 0.4116 - accuracy: 0.8613 - val_loss: 0.3684 - val_accuracy: 0.8804\n","Epoch 45/100\n","54/55 [============================>.] - ETA: 0s - loss: 0.3850 - accuracy: 0.8681\n","Epoch 45: val_loss improved from 0.35666 to 0.33538, saving model to saved_models/audio_classification.h5\n","55/55 [==============================] - 3s 63ms/step - loss: 0.3856 - accuracy: 0.8680 - val_loss: 0.3354 - val_accuracy: 0.8998\n","Epoch 46/100\n","54/55 [============================>.] - ETA: 0s - loss: 0.3890 - accuracy: 0.8675\n","Epoch 46: val_loss did not improve from 0.33538\n","55/55 [==============================] - 3s 62ms/step - loss: 0.3881 - accuracy: 0.8671 - val_loss: 0.3490 - val_accuracy: 0.8930\n","Epoch 47/100\n","55/55 [==============================] - ETA: 0s - loss: 0.3644 - accuracy: 0.8732\n","Epoch 47: val_loss did not improve from 0.33538\n","55/55 [==============================] - 5s 92ms/step - loss: 0.3644 - accuracy: 0.8732 - val_loss: 0.3455 - val_accuracy: 0.8987\n","Epoch 48/100\n","54/55 [============================>.] - ETA: 0s - loss: 0.3761 - accuracy: 0.8692\n","Epoch 48: val_loss improved from 0.33538 to 0.33052, saving model to saved_models/audio_classification.h5\n","55/55 [==============================] - 3s 59ms/step - loss: 0.3752 - accuracy: 0.8699 - val_loss: 0.3305 - val_accuracy: 0.9033\n","Epoch 49/100\n","55/55 [==============================] - ETA: 0s - loss: 0.3808 - accuracy: 0.8700\n","Epoch 49: val_loss improved from 0.33052 to 0.31535, saving model to saved_models/audio_classification.h5\n","55/55 [==============================] - 3s 59ms/step - loss: 0.3808 - accuracy: 0.8700 - val_loss: 0.3153 - val_accuracy: 0.9015\n","Epoch 50/100\n","54/55 [============================>.] - ETA: 0s - loss: 0.3587 - accuracy: 0.8772\n","Epoch 50: val_loss did not improve from 0.31535\n","55/55 [==============================] - 3s 62ms/step - loss: 0.3597 - accuracy: 0.8770 - val_loss: 0.3511 - val_accuracy: 0.8958\n","Epoch 51/100\n","55/55 [==============================] - ETA: 0s - loss: 0.3567 - accuracy: 0.8795\n","Epoch 51: val_loss did not improve from 0.31535\n","55/55 [==============================] - 5s 94ms/step - loss: 0.3567 - accuracy: 0.8795 - val_loss: 0.3162 - val_accuracy: 0.9038\n","Epoch 52/100\n","54/55 [============================>.] - ETA: 0s - loss: 0.3501 - accuracy: 0.8749\n","Epoch 52: val_loss did not improve from 0.31535\n","55/55 [==============================] - 3s 59ms/step - loss: 0.3494 - accuracy: 0.8753 - val_loss: 0.3248 - val_accuracy: 0.9044\n","Epoch 53/100\n","54/55 [============================>.] - ETA: 0s - loss: 0.3441 - accuracy: 0.8818\n","Epoch 53: val_loss improved from 0.31535 to 0.29477, saving model to saved_models/audio_classification.h5\n","55/55 [==============================] - 3s 62ms/step - loss: 0.3432 - accuracy: 0.8822 - val_loss: 0.2948 - val_accuracy: 0.9164\n","Epoch 54/100\n","54/55 [============================>.] - ETA: 0s - loss: 0.3487 - accuracy: 0.8789\n","Epoch 54: val_loss did not improve from 0.29477\n","55/55 [==============================] - 3s 60ms/step - loss: 0.3499 - accuracy: 0.8785 - val_loss: 0.2974 - val_accuracy: 0.9124\n","Epoch 55/100\n","55/55 [==============================] - ETA: 0s - loss: 0.3412 - accuracy: 0.8862\n","Epoch 55: val_loss did not improve from 0.29477\n","55/55 [==============================] - 5s 97ms/step - loss: 0.3412 - accuracy: 0.8862 - val_loss: 0.2978 - val_accuracy: 0.9061\n","Epoch 56/100\n","55/55 [==============================] - ETA: 0s - loss: 0.3412 - accuracy: 0.8865\n","Epoch 56: val_loss did not improve from 0.29477\n","55/55 [==============================] - 3s 59ms/step - loss: 0.3412 - accuracy: 0.8865 - val_loss: 0.3096 - val_accuracy: 0.9096\n","Epoch 57/100\n","54/55 [============================>.] - ETA: 0s - loss: 0.3394 - accuracy: 0.8831\n","Epoch 57: val_loss did not improve from 0.29477\n","55/55 [==============================] - 3s 59ms/step - loss: 0.3402 - accuracy: 0.8825 - val_loss: 0.3022 - val_accuracy: 0.8981\n","Epoch 58/100\n","54/55 [============================>.] - ETA: 0s - loss: 0.3156 - accuracy: 0.8928\n","Epoch 58: val_loss improved from 0.29477 to 0.28182, saving model to saved_models/audio_classification.h5\n","55/55 [==============================] - 3s 61ms/step - loss: 0.3167 - accuracy: 0.8923 - val_loss: 0.2818 - val_accuracy: 0.9159\n","Epoch 59/100\n","55/55 [==============================] - ETA: 0s - loss: 0.3217 - accuracy: 0.8895\n","Epoch 59: val_loss did not improve from 0.28182\n","55/55 [==============================] - 5s 95ms/step - loss: 0.3217 - accuracy: 0.8895 - val_loss: 0.3027 - val_accuracy: 0.9090\n","Epoch 60/100\n","54/55 [============================>.] - ETA: 0s - loss: 0.3190 - accuracy: 0.8903\n","Epoch 60: val_loss improved from 0.28182 to 0.27830, saving model to saved_models/audio_classification.h5\n","55/55 [==============================] - 4s 64ms/step - loss: 0.3180 - accuracy: 0.8909 - val_loss: 0.2783 - val_accuracy: 0.9176\n","Epoch 61/100\n","54/55 [============================>.] - ETA: 0s - loss: 0.3423 - accuracy: 0.8830\n","Epoch 61: val_loss did not improve from 0.27830\n","55/55 [==============================] - 3s 59ms/step - loss: 0.3399 - accuracy: 0.8839 - val_loss: 0.2856 - val_accuracy: 0.9084\n","Epoch 62/100\n","54/55 [============================>.] - ETA: 0s - loss: 0.2982 - accuracy: 0.9008\n","Epoch 62: val_loss did not improve from 0.27830\n","55/55 [==============================] - 3s 64ms/step - loss: 0.3000 - accuracy: 0.9002 - val_loss: 0.2792 - val_accuracy: 0.9181\n","Epoch 63/100\n","55/55 [==============================] - ETA: 0s - loss: 0.2927 - accuracy: 0.9011\n","Epoch 63: val_loss improved from 0.27830 to 0.27582, saving model to saved_models/audio_classification.h5\n","55/55 [==============================] - 5s 97ms/step - loss: 0.2927 - accuracy: 0.9011 - val_loss: 0.2758 - val_accuracy: 0.9193\n","Epoch 64/100\n","54/55 [============================>.] - ETA: 0s - loss: 0.2920 - accuracy: 0.8992\n","Epoch 64: val_loss improved from 0.27582 to 0.27448, saving model to saved_models/audio_classification.h5\n","55/55 [==============================] - 3s 61ms/step - loss: 0.2941 - accuracy: 0.8986 - val_loss: 0.2745 - val_accuracy: 0.9239\n","Epoch 65/100\n","54/55 [============================>.] - ETA: 0s - loss: 0.3076 - accuracy: 0.8997\n","Epoch 65: val_loss did not improve from 0.27448\n","55/55 [==============================] - 3s 59ms/step - loss: 0.3067 - accuracy: 0.8999 - val_loss: 0.2764 - val_accuracy: 0.9153\n","Epoch 66/100\n","54/55 [============================>.] - ETA: 0s - loss: 0.2834 - accuracy: 0.9036\n","Epoch 66: val_loss improved from 0.27448 to 0.26892, saving model to saved_models/audio_classification.h5\n","55/55 [==============================] - 4s 64ms/step - loss: 0.2831 - accuracy: 0.9038 - val_loss: 0.2689 - val_accuracy: 0.9256\n","Epoch 67/100\n","54/55 [============================>.] - ETA: 0s - loss: 0.2766 - accuracy: 0.9036\n","Epoch 67: val_loss improved from 0.26892 to 0.26512, saving model to saved_models/audio_classification.h5\n","55/55 [==============================] - 5s 96ms/step - loss: 0.2776 - accuracy: 0.9031 - val_loss: 0.2651 - val_accuracy: 0.9222\n","Epoch 68/100\n","54/55 [============================>.] - ETA: 0s - loss: 0.2634 - accuracy: 0.9120\n","Epoch 68: val_loss did not improve from 0.26512\n","55/55 [==============================] - 3s 60ms/step - loss: 0.2630 - accuracy: 0.9117 - val_loss: 0.2678 - val_accuracy: 0.9250\n","Epoch 69/100\n","54/55 [============================>.] - ETA: 0s - loss: 0.2918 - accuracy: 0.9031\n","Epoch 69: val_loss did not improve from 0.26512\n","55/55 [==============================] - 3s 62ms/step - loss: 0.2913 - accuracy: 0.9032 - val_loss: 0.2923 - val_accuracy: 0.9096\n","Epoch 70/100\n","55/55 [==============================] - ETA: 0s - loss: 0.2683 - accuracy: 0.9059\n","Epoch 70: val_loss did not improve from 0.26512\n","55/55 [==============================] - 4s 67ms/step - loss: 0.2683 - accuracy: 0.9059 - val_loss: 0.2753 - val_accuracy: 0.9199\n","Epoch 71/100\n","54/55 [============================>.] - ETA: 0s - loss: 0.2579 - accuracy: 0.9145\n","Epoch 71: val_loss did not improve from 0.26512\n","55/55 [==============================] - 5s 89ms/step - loss: 0.2572 - accuracy: 0.9148 - val_loss: 0.2783 - val_accuracy: 0.9187\n","Epoch 72/100\n","54/55 [============================>.] - ETA: 0s - loss: 0.2720 - accuracy: 0.9100\n","Epoch 72: val_loss did not improve from 0.26512\n","55/55 [==============================] - 3s 63ms/step - loss: 0.2719 - accuracy: 0.9099 - val_loss: 0.2950 - val_accuracy: 0.9204\n","Epoch 73/100\n","54/55 [============================>.] - ETA: 0s - loss: 0.2722 - accuracy: 0.9091\n","Epoch 73: val_loss did not improve from 0.26512\n","55/55 [==============================] - 3s 61ms/step - loss: 0.2719 - accuracy: 0.9092 - val_loss: 0.2721 - val_accuracy: 0.9239\n","Epoch 74/100\n","55/55 [==============================] - ETA: 0s - loss: 0.2433 - accuracy: 0.9183\n","Epoch 74: val_loss did not improve from 0.26512\n","55/55 [==============================] - 4s 67ms/step - loss: 0.2433 - accuracy: 0.9183 - val_loss: 0.2760 - val_accuracy: 0.9279\n","Epoch 75/100\n","54/55 [============================>.] - ETA: 0s - loss: 0.2462 - accuracy: 0.9171\n","Epoch 75: val_loss improved from 0.26512 to 0.26111, saving model to saved_models/audio_classification.h5\n","55/55 [==============================] - 5s 90ms/step - loss: 0.2462 - accuracy: 0.9171 - val_loss: 0.2611 - val_accuracy: 0.9256\n","Epoch 76/100\n","55/55 [==============================] - ETA: 0s - loss: 0.2587 - accuracy: 0.9138\n","Epoch 76: val_loss improved from 0.26111 to 0.24670, saving model to saved_models/audio_classification.h5\n","55/55 [==============================] - 3s 63ms/step - loss: 0.2587 - accuracy: 0.9138 - val_loss: 0.2467 - val_accuracy: 0.9273\n","Epoch 77/100\n","55/55 [==============================] - ETA: 0s - loss: 0.2463 - accuracy: 0.9214\n","Epoch 77: val_loss improved from 0.24670 to 0.24117, saving model to saved_models/audio_classification.h5\n","55/55 [==============================] - 4s 66ms/step - loss: 0.2463 - accuracy: 0.9214 - val_loss: 0.2412 - val_accuracy: 0.9307\n","Epoch 78/100\n","55/55 [==============================] - ETA: 0s - loss: 0.2413 - accuracy: 0.9185\n","Epoch 78: val_loss did not improve from 0.24117\n","55/55 [==============================] - 5s 92ms/step - loss: 0.2413 - accuracy: 0.9185 - val_loss: 0.2467 - val_accuracy: 0.9262\n","Epoch 79/100\n","54/55 [============================>.] - ETA: 0s - loss: 0.2497 - accuracy: 0.9123\n","Epoch 79: val_loss did not improve from 0.24117\n","55/55 [==============================] - 4s 78ms/step - loss: 0.2496 - accuracy: 0.9124 - val_loss: 0.2622 - val_accuracy: 0.9227\n","Epoch 80/100\n","54/55 [============================>.] - ETA: 0s - loss: 0.2425 - accuracy: 0.9185\n","Epoch 80: val_loss did not improve from 0.24117\n","55/55 [==============================] - 3s 63ms/step - loss: 0.2426 - accuracy: 0.9185 - val_loss: 0.2564 - val_accuracy: 0.9284\n","Epoch 81/100\n","54/55 [============================>.] - ETA: 0s - loss: 0.2356 - accuracy: 0.9239\n","Epoch 81: val_loss did not improve from 0.24117\n","55/55 [==============================] - 3s 64ms/step - loss: 0.2348 - accuracy: 0.9237 - val_loss: 0.2481 - val_accuracy: 0.9302\n","Epoch 82/100\n","55/55 [==============================] - ETA: 0s - loss: 0.2622 - accuracy: 0.9094\n","Epoch 82: val_loss did not improve from 0.24117\n","55/55 [==============================] - 4s 80ms/step - loss: 0.2622 - accuracy: 0.9094 - val_loss: 0.2453 - val_accuracy: 0.9296\n","Epoch 83/100\n","54/55 [============================>.] - ETA: 0s - loss: 0.2372 - accuracy: 0.9219\n","Epoch 83: val_loss improved from 0.24117 to 0.23584, saving model to saved_models/audio_classification.h5\n","55/55 [==============================] - 5s 82ms/step - loss: 0.2373 - accuracy: 0.9218 - val_loss: 0.2358 - val_accuracy: 0.9359\n","Epoch 84/100\n","54/55 [============================>.] - ETA: 0s - loss: 0.2496 - accuracy: 0.9196\n","Epoch 84: val_loss did not improve from 0.23584\n","55/55 [==============================] - 4s 65ms/step - loss: 0.2486 - accuracy: 0.9198 - val_loss: 0.2476 - val_accuracy: 0.9239\n","Epoch 85/100\n","54/55 [============================>.] - ETA: 0s - loss: 0.2285 - accuracy: 0.9184\n","Epoch 85: val_loss improved from 0.23584 to 0.22508, saving model to saved_models/audio_classification.h5\n","55/55 [==============================] - 4s 65ms/step - loss: 0.2293 - accuracy: 0.9181 - val_loss: 0.2251 - val_accuracy: 0.9336\n","Epoch 86/100\n","55/55 [==============================] - ETA: 0s - loss: 0.2198 - accuracy: 0.9253\n","Epoch 86: val_loss did not improve from 0.22508\n","55/55 [==============================] - 5s 86ms/step - loss: 0.2198 - accuracy: 0.9253 - val_loss: 0.2498 - val_accuracy: 0.9256\n","Epoch 87/100\n","55/55 [==============================] - ETA: 0s - loss: 0.2331 - accuracy: 0.9207\n","Epoch 87: val_loss did not improve from 0.22508\n","55/55 [==============================] - 4s 70ms/step - loss: 0.2331 - accuracy: 0.9207 - val_loss: 0.2440 - val_accuracy: 0.9313\n","Epoch 88/100\n","54/55 [============================>.] - ETA: 0s - loss: 0.2241 - accuracy: 0.9261\n","Epoch 88: val_loss did not improve from 0.22508\n","55/55 [==============================] - 4s 64ms/step - loss: 0.2235 - accuracy: 0.9261 - val_loss: 0.2573 - val_accuracy: 0.9284\n","Epoch 89/100\n","54/55 [============================>.] - ETA: 0s - loss: 0.2297 - accuracy: 0.9219\n","Epoch 89: val_loss did not improve from 0.22508\n","55/55 [==============================] - 3s 61ms/step - loss: 0.2300 - accuracy: 0.9217 - val_loss: 0.2444 - val_accuracy: 0.9267\n","Epoch 90/100\n","55/55 [==============================] - ETA: 0s - loss: 0.2115 - accuracy: 0.9276\n","Epoch 90: val_loss did not improve from 0.22508\n","55/55 [==============================] - 5s 87ms/step - loss: 0.2115 - accuracy: 0.9276 - val_loss: 0.2388 - val_accuracy: 0.9307\n","Epoch 91/100\n","54/55 [============================>.] - ETA: 0s - loss: 0.2426 - accuracy: 0.9209\n","Epoch 91: val_loss did not improve from 0.22508\n","55/55 [==============================] - 4s 70ms/step - loss: 0.2433 - accuracy: 0.9205 - val_loss: 0.2551 - val_accuracy: 0.9273\n","Epoch 92/100\n","54/55 [============================>.] - ETA: 0s - loss: 0.2460 - accuracy: 0.9258\n","Epoch 92: val_loss did not improve from 0.22508\n","55/55 [==============================] - 3s 63ms/step - loss: 0.2470 - accuracy: 0.9256 - val_loss: 0.2309 - val_accuracy: 0.9325\n","Epoch 93/100\n","54/55 [============================>.] - ETA: 0s - loss: 0.2218 - accuracy: 0.9259\n","Epoch 93: val_loss did not improve from 0.22508\n","55/55 [==============================] - 3s 63ms/step - loss: 0.2216 - accuracy: 0.9258 - val_loss: 0.2544 - val_accuracy: 0.9250\n","Epoch 94/100\n","55/55 [==============================] - ETA: 0s - loss: 0.2332 - accuracy: 0.9263\n","Epoch 94: val_loss did not improve from 0.22508\n","55/55 [==============================] - 5s 91ms/step - loss: 0.2332 - accuracy: 0.9263 - val_loss: 0.2348 - val_accuracy: 0.9359\n","Epoch 95/100\n","54/55 [============================>.] - ETA: 0s - loss: 0.2225 - accuracy: 0.9291\n","Epoch 95: val_loss improved from 0.22508 to 0.22046, saving model to saved_models/audio_classification.h5\n","55/55 [==============================] - 4s 68ms/step - loss: 0.2233 - accuracy: 0.9288 - val_loss: 0.2205 - val_accuracy: 0.9399\n","Epoch 96/100\n","55/55 [==============================] - ETA: 0s - loss: 0.2277 - accuracy: 0.9236\n","Epoch 96: val_loss did not improve from 0.22046\n","55/55 [==============================] - 3s 64ms/step - loss: 0.2277 - accuracy: 0.9236 - val_loss: 0.2307 - val_accuracy: 0.9353\n","Epoch 97/100\n","54/55 [============================>.] - ETA: 0s - loss: 0.2260 - accuracy: 0.9220\n","Epoch 97: val_loss did not improve from 0.22046\n","55/55 [==============================] - 3s 61ms/step - loss: 0.2246 - accuracy: 0.9227 - val_loss: 0.2238 - val_accuracy: 0.9319\n","Epoch 98/100\n","55/55 [==============================] - ETA: 0s - loss: 0.2024 - accuracy: 0.9310\n","Epoch 98: val_loss did not improve from 0.22046\n","55/55 [==============================] - 5s 93ms/step - loss: 0.2024 - accuracy: 0.9310 - val_loss: 0.2443 - val_accuracy: 0.9279\n","Epoch 99/100\n","54/55 [============================>.] - ETA: 0s - loss: 0.2342 - accuracy: 0.9255\n","Epoch 99: val_loss did not improve from 0.22046\n","55/55 [==============================] - 3s 62ms/step - loss: 0.2361 - accuracy: 0.9251 - val_loss: 0.2260 - val_accuracy: 0.9342\n","Epoch 100/100\n","55/55 [==============================] - ETA: 0s - loss: 0.2149 - accuracy: 0.9278\n","Epoch 100: val_loss did not improve from 0.22046\n","55/55 [==============================] - 3s 61ms/step - loss: 0.2149 - accuracy: 0.9278 - val_loss: 0.2410 - val_accuracy: 0.9353\n","Training completed in time:  0:07:23.386029\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"imt4Mdr1Z_UA","outputId":"1604e980-2f50-42b9-803c-8b21e34fe897","executionInfo":{"status":"ok","timestamp":1679815099269,"user_tz":-330,"elapsed":1366,"user":{"displayName":"Amanwit Kumar","userId":"16746267613797614089"}}},"source":["test_accuracy=model.evaluate(X_test,y_test,verbose=0)\n","print(test_accuracy[1])"],"id":"imt4Mdr1Z_UA","execution_count":58,"outputs":[{"output_type":"stream","name":"stdout","text":["0.9353176951408386\n"]}]},{"cell_type":"markdown","metadata":{"id":"fQAryFEqihSU"},"source":["### Testing Some NEW Audio Data"],"id":"fQAryFEqihSU"},{"cell_type":"code","metadata":{"id":"YJ_81Cfyieaz","executionInfo":{"status":"ok","timestamp":1679815492317,"user_tz":-330,"elapsed":1213,"user":{"displayName":"Amanwit Kumar","userId":"16746267613797614089"}}},"source":["filename=\"/content/drive/MyDrive/Capstone/UrbanSound8K/sample/mixkit-ambulance-siren-uk-1640.wav\"\n","audio, sample_rate = librosa.load(filename, sr=None, res_type='kaiser_fast') \n","mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n","mfccs_scaled_features = np.mean(mfccs_features.T,axis=0)"],"id":"YJ_81Cfyieaz","execution_count":68,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lM3n3nUQjqxU","outputId":"ec65e74f-643c-4187-8b45-8f382a1766c3","executionInfo":{"status":"ok","timestamp":1679815495371,"user_tz":-330,"elapsed":719,"user":{"displayName":"Amanwit Kumar","userId":"16746267613797614089"}}},"source":["mfccs_scaled_features.shape"],"id":"lM3n3nUQjqxU","execution_count":69,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(40,)"]},"metadata":{},"execution_count":69}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":87},"id":"2gBAQPEvjg4V","outputId":"e40269e0-954a-4211-b65c-2b4713c15b61","executionInfo":{"status":"ok","timestamp":1679815500283,"user_tz":-330,"elapsed":7,"user":{"displayName":"Amanwit Kumar","userId":"16746267613797614089"}}},"source":["mfccs_scaled_features=mfccs_scaled_features.reshape(1,-1)\n","print(mfccs_scaled_features.shape)\n","predicted_label = np.argmax(model.predict(mfccs_scaled_features), axis=-1)\n","print('Predicted Label:',predicted_label)\n","prediction_class = labelencoder.inverse_transform(predicted_label) \n","prediction_class[0]"],"id":"2gBAQPEvjg4V","execution_count":70,"outputs":[{"output_type":"stream","name":"stdout","text":["(1, 40)\n","1/1 [==============================] - 0s 26ms/step\n","Predicted Label: [8]\n"]},{"output_type":"execute_result","data":{"text/plain":["'siren'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":70}]},{"cell_type":"code","metadata":{"id":"P1lg60Knoe7d","executionInfo":{"status":"ok","timestamp":1679815332355,"user_tz":-330,"elapsed":1,"user":{"displayName":"Amanwit Kumar","userId":"16746267613797614089"}}},"source":[],"id":"P1lg60Knoe7d","execution_count":67,"outputs":[]}]}